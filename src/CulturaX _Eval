

# Force cleanup
gc.collect()
if torch.cuda.is_available():
    torch.cuda.synchronize()
    torch.cuda.empty_cache()
    # Reset CUDA context
    torch.cuda.reset_peak_memory_stats()
    torch.cuda.reset_accumulated_memory_stats()

print("CUDA cleaned. Now run the main code below.")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB")


import os
import torch
import torch.nn as nn
import numpy as np
import fasttext
import gc
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from huggingface_hub import login

# 1. GPU & DEBUG SETTINGS
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
SOURCE_ID = "EleutherAI/pythia-1b" 
TARGET_ID = "Qwen/Qwen2-1.5B"

def get_relrep_mapping(src_tok, tgt_tok):
    print("Step 1: Building Semantic Map (CPU)...")
    def get_vecs(tokenizer, name):
        ds = load_dataset("uonlp/CulturaX", "en", split="train", streaming=True).take(2000)
        with open(f"{name}_map.txt", "w", encoding="utf-8") as f:
            for x in ds: 
                f.write(x['text'] + "\n")
        ft = fasttext.train_unsupervised(f"{name}_map.txt", model='skipgram', dim=100, verbose=0)
        
        vecs = []
        for i in range(len(tokenizer)):
            try:
                token_str = tokenizer.decode([i])
                vec = ft.get_word_vector(token_str)
                vecs.append(vec)
            except:
                vecs.append(np.zeros(100))
        return np.array(vecs)

    s_v = get_vecs(src_tok, "src")
    t_v = get_vecs(tgt_tok, "tgt")
    
    # Get vocab sizes
    src_vocab_size = len(src_tok)
    tgt_vocab_size = len(tgt_tok)
    
    print(f"Source vocab size: {src_vocab_size}")
    print(f"Target vocab size: {tgt_vocab_size}")
    
    # Build anchors
    src_vocab = src_tok.get_vocab()
    tgt_vocab = tgt_tok.get_vocab()
    common = list(set(src_vocab.keys()) & set(tgt_vocab.keys()))
    anchors = sorted([t for t in common if len(t) > 2 and t.isalpha()])[:300]
    
    if len(anchors) < 10:
        print("Warning: Few anchors, using simple mapping")
        return np.minimum(np.arange(src_vocab_size), tgt_vocab_size - 1)
    
    s_a = [src_vocab[t] for t in anchors]
    t_a = [tgt_vocab[t] for t in anchors]
    
    src_rel = np.dot(s_v, s_v[s_a].T) / (np.linalg.norm(s_v, axis=1, keepdims=True) + 1e-10)
    tgt_rel = np.dot(t_v, t_v[t_a].T) / (np.linalg.norm(t_v, axis=1, keepdims=True) + 1e-10)
    
    mapping = np.argmax(np.dot(src_rel, tgt_rel.T), axis=1)
    mapping = np.clip(mapping, 0, tgt_vocab_size - 1)
    
    print(f"Mapping range: [{mapping.min()}, {mapping.max()}]")
    return mapping

def evaluate_normalized_ppl(model, tgt_tok, src_tok, lang, n_samples=50):
    model.eval()
    total_nll = 0.0
    total_src_tokens = 0
    stride = 512
    max_length = 512

    try:
        ds = load_dataset("uonlp/CulturaX", lang, split="train", streaming=True)
    except:
        print(f"Cannot load dataset for {lang}")
        return float('inf')
    
    count = 0
    
    for entry in tqdm(ds, desc=f"Eval {lang.upper()}", total=n_samples):
        if count >= n_samples: 
            break
        
        text = entry.get('text', '')
        if not text or len(text.strip()) < 10: 
            continue

        try:
            enc = tgt_tok(text, return_tensors="pt", truncation=True, max_length=2048)
            seq_len = enc.input_ids.size(1)
            
            doc_nll = 0.0
            for begin_loc in range(0, seq_len, stride):
                end_loc = min(begin_loc + max_length, seq_len)
                target_ids = enc.input_ids[:, begin_loc:end_loc].to(DEVICE)
                
                if target_ids.size(1) <= 1: 
                    continue

                with torch.no_grad():
                    outputs = model(target_ids, labels=target_ids)
                    doc_nll += outputs.loss.item() * (target_ids.size(1) - 1)

            # Normalization
            py_enc = src_tok(text, return_tensors="pt", truncation=True, max_length=2048)
            py_count = py_enc.input_ids.size(1) - 1
            
            if py_count > 0:
                total_src_tokens += py_count
                total_nll += doc_nll
                count += 1
                
            if count % 10 == 0: 
                torch.cuda.empty_cache()
                
        except Exception as e:
            print(f"Sample error: {e}")
            continue

    return np.exp(total_nll / total_src_tokens) if total_src_tokens > 0 else float('inf')

def main():
    login(token="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")
    
    print("\n" + "="*60)
    print("CROSS-MODEL TOKEN TRANSFER")
    print("="*60)
    
    # Load tokenizers
    print("\nLoading tokenizers...")
    src_tok = AutoTokenizer.from_pretrained(SOURCE_ID)
    tgt_tok = AutoTokenizer.from_pretrained(TARGET_ID)
    if tgt_tok.pad_token is None: 
        tgt_tok.pad_token = tgt_tok.eos_token

    src_vocab_size = len(src_tok)
    tgt_vocab_size = len(tgt_tok)
    
    print(f"Source vocab: {src_vocab_size}")
    print(f"Target vocab: {tgt_vocab_size}")

    # Load source model (CPU only)
    print("\n" + "-"*60)
    print("Loading source model on CPU...")
    s_model = AutoModelForCausalLM.from_pretrained(
        SOURCE_ID, 
        torch_dtype=torch.float32,
        low_cpu_mem_usage=True
    )
    # Keep on CPU - don't move to CUDA
    src_weights = s_model.get_input_embeddings().weight.data.cpu().clone()
    src_embed_dim = src_weights.shape[1]
    print(f"Source embedding dim: {src_embed_dim}")
    
    # Get mapping
    token_map = get_relrep_mapping(src_tok, tgt_tok)
    
    # Cleanup source model
    del s_model
    gc.collect()

    # Load target model
    print("\n" + "-"*60)
    print("Loading target model on GPU...")
    t_model = AutoModelForCausalLM.from_pretrained(
        TARGET_ID, 
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True
    ).to(DEVICE)
    
    target_embed_dim = t_model.get_input_embeddings().weight.shape[1]
    target_vocab_size_actual = t_model.get_input_embeddings().weight.shape[0]
    print(f"Target embedding dim: {target_embed_dim}")
    print(f"Target vocab (actual): {target_vocab_size_actual}")

    # Apply projection
    print("\n" + "-"*60)
    print("Projecting embeddings...")
    
    with torch.no_grad():
        # Create projection layer ON GPU
        proj = nn.Linear(src_embed_dim, target_embed_dim, bias=False).to(DEVICE).to(torch.bfloat16)
        
        # Process in batches to avoid memory issues
        batch_size = 1000
        target_embeddings = t_model.get_input_embeddings().weight.data
        
        for start_idx in tqdm(range(0, len(token_map), batch_size), desc="Transferring"):
            end_idx = min(start_idx + batch_size, len(token_map))
            
            # Get batch of source embeddings
            batch_src = src_weights[start_idx:end_idx].to(DEVICE).to(torch.bfloat16)
            
            # Project
            batch_proj = proj(batch_src)
            
            # Map to target positions
            for i, src_idx in enumerate(range(start_idx, end_idx)):
                tgt_idx = token_map[src_idx]
                if 0 <= tgt_idx < target_vocab_size_actual:
                    target_embeddings[tgt_idx] = batch_proj[i]
            
            # Clear batch
            del batch_src, batch_proj
            
        # Tie output layer if applicable
        if hasattr(t_model, "lm_head"):
            if t_model.lm_head.weight.shape == target_embeddings.shape:
                t_model.lm_head.weight.data = target_embeddings
    
    print("Transfer complete!")
    
    # Cleanup
    del src_weights, proj
    gc.collect()
    torch.cuda.empty_cache()

    # Evaluation
    print("\n" + "="*60)
    print("EVALUATING PERPLEXITY")
    print("="*60)
    langs = ["ar", "de", "en", "ja", "zh", "bn", "ko", "th", "uk", "vi", "ta", "te", "ur"]  # ALL LANGUAGES
    
    print(f"\n{'LANG':<10} | {'NORM PPL':>12}")
    print("-"*60)
    
    for lang in langs:
        try:
            ppl = evaluate_normalized_ppl(t_model, tgt_tok, src_tok, lang, n_samples=50)
            print(f"{lang.upper():<10} | {ppl:12.4f}")
        except Exception as e:
            print(f"{lang.upper():<10} | ERROR: {str(e)[:40]}")
        
        gc.collect()
        torch.cuda.empty_cache()

    print("\n" + "="*60)
    print("COMPLETE")
    print("="*60)

if __name__ == "__main__":
    main()
